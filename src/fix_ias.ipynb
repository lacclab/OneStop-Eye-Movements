{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "base_path = Path(\n",
    "    \"/Users/shubi/Library/CloudStorage/OneDrive-Technion/In-lab Experiments/OneStopGaze L1 English\"\n",
    ")\n",
    "exp_folders_path = base_path / \"Full Experiment Folders\"\n",
    "\n",
    "\n",
    "def get_missing_words(p_words: list[str], p_ias: pd.DataFrame) -> list[str]:\n",
    "    missing_words = []\n",
    "    for idx, word in enumerate(p_words[::-1]):\n",
    "        idx = len(p_words) - idx - 1\n",
    "        if idx >= len(p_ias):\n",
    "            # print(\n",
    "            #     f\"Error: {ias_name} has {len(p_ias)} AOIs, but the trial has {len(p_words)} words\"\n",
    "            # )\n",
    "            missing_words.append(word)\n",
    "        elif abs(len(p_ias) - len(p_words)) > 10:\n",
    "            raise Exception\n",
    "        else:\n",
    "            break\n",
    "\n",
    "    missing_words = list(reversed(missing_words))\n",
    "    return missing_words\n",
    "\n",
    "\n",
    "def add_missing_rows_to_ias(\n",
    "    p_ias: pd.DataFrame, missing_words: list[str]\n",
    ") -> pd.DataFrame:\n",
    "    char_width = 19\n",
    "    margin = 10\n",
    "    for indx, word in enumerate(missing_words):\n",
    "        num_characters = len(word)\n",
    "        word_len_in_px = (num_characters * char_width) + (margin * 2) - 1\n",
    "        new_row = {\n",
    "            \"group\": None,\n",
    "            \"type\": None,\n",
    "            \"ID\": p_ias[\"ID\"].iloc[-1] + 1,\n",
    "            \"label\": word,\n",
    "        }\n",
    "\n",
    "        if indx == 0:\n",
    "            new_row.update(\n",
    "                {\n",
    "                    \"left\": 358,\n",
    "                    \"top\": p_ias[\"bottom\"].iloc[-1],\n",
    "                    \"right\": 358 + word_len_in_px,\n",
    "                    \"bottom\": p_ias[\"bottom\"].iloc[-1] + 114,\n",
    "                }\n",
    "            )\n",
    "        else:\n",
    "            new_row.update(\n",
    "                {\n",
    "                    \"left\": p_ias[\"right\"].iloc[-1],\n",
    "                    \"top\": p_ias[\"top\"].iloc[-1],\n",
    "                    \"right\": p_ias[\"right\"].iloc[-1] + word_len_in_px,\n",
    "                    \"bottom\": p_ias[\"bottom\"].iloc[-1],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        new_row = pd.Series(new_row)\n",
    "        new_row[[\"group\", \"type\"]] = p_ias[[\"group\", \"type\"]].ffill(axis=0).iloc[-1]\n",
    "        p_ias = pd.concat([p_ias, new_row.to_frame().T], ignore_index=True)\n",
    "    return p_ias\n",
    "\n",
    "\n",
    "def load_ias(ias_path: Path) -> pd.DataFrame:\n",
    "    ias_data = pd.read_csv(\n",
    "        ias_path,\n",
    "        sep=\"\\t\",\n",
    "        names=[\"group\", \"ID\", \"left\", \"top\", \"right\", \"bottom\", \"label\"],\n",
    "    )\n",
    "    ias_data[[\"group\", \"type\"]] = ias_data[\"group\"].str.split(\" \", n=1, expand=True)\n",
    "    ias_data = ias_data[\n",
    "        [\"group\", \"type\", \"ID\", \"left\", \"top\", \"right\", \"bottom\", \"label\"]\n",
    "    ]\n",
    "    return ias_data  # type: ignore\n",
    "\n",
    "\n",
    "def get_paragarph_ias(ias_data: pd.DataFrame, subject_id: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Get the largest group size from the given IAS data.\n",
    "    Does not take into account the last 3 groups as they are question, question+answers, feedback.\n",
    "\n",
    "    Args:\n",
    "        ias_data (pd.DataFrame): The input IAS data.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The largest group.\n",
    "    \"\"\"\n",
    "\n",
    "    # Extract a list of all the 'group' values in ias_data\n",
    "    group_values = ias_data[\"group\"].unique().tolist()\n",
    "    # discard the last three\n",
    "    if len(group_values) != 1:\n",
    "        group_values = group_values[:-3]\n",
    "        # keep only ias_data where group is in group_values\n",
    "        ias_data = ias_data[ias_data[\"group\"].isin(group_values)].copy()  # type: ignore\n",
    "    else:\n",
    "        assert subject_id == \"l34_277\"  # IA_40.ias (trial 38) has only one group.\n",
    "\n",
    "    largest_group = ias_data.groupby(\"group\").size().idxmax()\n",
    "    ias_data_largest_group = ias_data[ias_data[\"group\"] == largest_group]\n",
    "    return ias_data_largest_group  # type: ignore\n",
    "\n",
    "\n",
    "def load_dat(dat_file_path: Path, names=None) -> pd.DataFrame:\n",
    "    if names:\n",
    "        dat = pd.read_csv(dat_file_path, sep=\"\\t\", names=names)\n",
    "    else:\n",
    "        dat = pd.read_csv(dat_file_path, sep=\"\\t\")\n",
    "    return dat\n",
    "\n",
    "\n",
    "def get_trial_index(ias_path: Path, trial_report: pd.DataFrame, subject_id) -> int:\n",
    "    trial_index = ias_path.name.split(\"_\")[1].split(\".\")[0]\n",
    "    actual_trial_indx = (  # type: ignore\n",
    "        trial_report.loc[\n",
    "            (trial_report[\"RECORDING_SESSION_LABEL\"] == subject_id)\n",
    "            & (trial_report[\"Trial_Index_\"] == trial_index),\n",
    "            \"trial\",\n",
    "        ]\n",
    "        .astype(int)\n",
    "        .item()\n",
    "    )\n",
    "    return actual_trial_indx\n",
    "\n",
    "\n",
    "def get_parag_words(\n",
    "    subject_dat, ias_path, trial_report: pd.DataFrame, subject_id\n",
    ") -> list[str]:\n",
    "    trial_dat = subject_dat[\n",
    "        subject_dat[\"trial\"] == get_trial_index(ias_path, trial_report, subject_id)\n",
    "    ].drop_duplicates()\n",
    "    p_words = trial_dat[\"$paragraph\"].item().split()\n",
    "    return p_words\n",
    "\n",
    "\n",
    "def get_dat_file_path(exp_folders_path: Path, folder_name: str) -> Path:\n",
    "    dat_file_names = [\n",
    "        \"TRIAL_DataSource_onestop_BLOCKTRIAL.dat\",\n",
    "        f\"TRIAL_DataSource_{folder_name}_BLOCKTRIAL.dat\",\n",
    "        f\"TRIAL_DataSource_{folder_name.removesuffix('_widthfix')}_BLOCKTRIAL.dat\",\n",
    "        f\"TRIAL_DataSource_{folder_name.removesuffix('_l2_latest')}_BLOCKTRIAL.dat\",\n",
    "    ]\n",
    "    # TRIAL_DataSource_ose_1p_l1_l60_tower_st_mit_l1_latest_widthfix\n",
    "    # TRIAL_DataSource_ose_1p_l1_l60_tower_st_mit_l1_latest_BLOCKTRIAL\n",
    "    for file_name in dat_file_names:\n",
    "        dat_file_path = exp_folders_path / folder_name / \"datasets\" / file_name\n",
    "        if dat_file_path.exists():\n",
    "            return dat_file_path\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Could not find the dat file for {folder_name}\")\n",
    "\n",
    "\n",
    "def get_subject_dat_file_path(exp_folders_path, folder_name, subject_id):\n",
    "    dat_file_names = [\n",
    "        \"actual_TRIAL_DataSource_onestop_BLOCKTRIAL.dat\",\n",
    "        f\"actual_TRIAL_DataSource_{folder_name}_BLOCKTRIAL.dat\",\n",
    "        f\"actual_TRIAL_DataSource_{folder_name.removesuffix('_widthfix')}_BLOCKTRIAL.dat\",\n",
    "        f\"actual_TRIAL_DataSource_{folder_name.removesuffix('_l2_latest')}_BLOCKTRIAL.dat\",\n",
    "    ]\n",
    "    # actual_TRIAL_DataSource_ose_1p_l1_l60_tower_st_mit_l1_latest_BLOCKTRIAL\n",
    "    for file_name in dat_file_names:\n",
    "        subject_dat_file_path = (\n",
    "            exp_folders_path / folder_name / \"results\" / subject_id / file_name\n",
    "        )\n",
    "        if subject_dat_file_path.exists():\n",
    "            return subject_dat_file_path\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find the dat file for {subject_id}\")\n",
    "\n",
    "\n",
    "def get_aoi_folder_path(exp_folders_path, folder_name, subject_id):\n",
    "    potential_paths = [\n",
    "        exp_folders_path / folder_name / \"results\" / subject_id / \"aoi\",\n",
    "        exp_folders_path / folder_name / \"runtime\" / \"dataviewer\" / subject_id / \"aoi\",\n",
    "    ]\n",
    "\n",
    "    for path in potential_paths:\n",
    "        if path.exists():\n",
    "            return path\n",
    "\n",
    "    raise FileNotFoundError(f\"Could not find the aoi folder for {subject_id}\")\n",
    "\n",
    "\n",
    "def rebuild_ias_data(ias_data, p_ias) -> pd.DataFrame:\n",
    "    group_value = p_ias.group.unique().item()\n",
    "\n",
    "    before = ias_data[\n",
    "        ias_data.index < ias_data[ias_data[\"group\"] == group_value].index.min()\n",
    "    ]\n",
    "    after = ias_data[\n",
    "        ias_data.index > ias_data[ias_data[\"group\"] == group_value].index.max()\n",
    "    ]\n",
    "\n",
    "    ias_data = pd.concat([before, p_ias, after], ignore_index=True)\n",
    "\n",
    "    # Original ias file has tabs as separators except for group and type which are separated by a space.\n",
    "    ias_data[\"group\"] = ias_data[\"group\"] + \" \" + ias_data[\"type\"]\n",
    "    ias_data = ias_data.drop(columns=[\"type\"])\n",
    "\n",
    "    return ias_data\n",
    "\n",
    "\n",
    "def get_sorted_ias_paths(\n",
    "    exp_folders_path: Path,\n",
    "    folder_name: str,\n",
    "    subject_id: str,\n",
    "    trial_report: pd.DataFrame,\n",
    ") -> list[Path]:\n",
    "    aoi_folder = get_aoi_folder_path(exp_folders_path, folder_name, subject_id)\n",
    "    ias_paths = list(aoi_folder.glob(\"*.ias\"))\n",
    "    ias_paths.sort(\n",
    "        key=lambda path: get_trial_index(\n",
    "            path, trial_report=trial_report, subject_id=subject_id\n",
    "        )\n",
    "    )\n",
    "    return ias_paths\n",
    "\n",
    "\n",
    "def load_subject_dat(\n",
    "    exp_folders_path: Path, folder_name: str, subject_id: str\n",
    ") -> pd.DataFrame:\n",
    "    dat_file_path = get_dat_file_path(exp_folders_path, folder_name)\n",
    "    dat = load_dat(dat_file_path)\n",
    "\n",
    "    subject_dat_file_path = get_subject_dat_file_path(\n",
    "        exp_folders_path, folder_name, subject_id\n",
    "    )\n",
    "    subject_dat = load_dat(subject_dat_file_path, names=dat.columns.to_list())\n",
    "\n",
    "    return subject_dat\n",
    "\n",
    "\n",
    "def handle_long_words_going_down_line(\n",
    "    p_ias: pd.DataFrame, folder_name, subject_id, ias_path, level: str\n",
    ") -> tuple[pd.DataFrame, bool]:\n",
    "    # Find duplicate consecutive words in p_ias[\"label\"] by shifting the column by one and comparing\n",
    "    # with the original column. If the two consecutive words are the same, then the word is a duplicate.\n",
    "    duplicates = p_ias[\"label\"].shift(-1) == p_ias[\"label\"]\n",
    "\n",
    "    found_duplicates = True if duplicates.any() else False\n",
    "\n",
    "    word_parts = {\n",
    "        \"south-east\": [\"south-\", \"east\"],\n",
    "        \"credit-card\": [\"credit-\", \"card\"],\n",
    "        \"hunter-gatherer\": [\"hunter-\", \"gatherer\"],\n",
    "        \"100sq-meter\": [\"100sq-\", \"meter\"],\n",
    "        \"brand-new\": [\"brand-\", \"new\"],\n",
    "        \"deep-fried\": [\"deep-\", \"fried\"],\n",
    "        \"e-bicycles\": [\"e-\", \"bicycles\"],\n",
    "        \"film-editing\": [\"film-\", \"editing\"],\n",
    "        \"open-minded,\": [\"open-\", \"minded,\"],\n",
    "        \"three-square-meter\": {\n",
    "            \"Adv\": [\"three-square-\", \"meter\"],\n",
    "            \"Ele\": [\"three-\", \"square-meter\"],\n",
    "        },\n",
    "        \"French-Canadian\": [\"French-\", \"Canadian\"],\n",
    "        \"honey-flavored\": [\"honey-\", \"flavored\"],\n",
    "        \"top-level\": [\"top-\", \"level\"],\n",
    "        \"film-makers.\": [\"film-\", \"makers.\"],\n",
    "        \"post-genocide\": [\"post-\", \"genocide\"],\n",
    "        \"10-year-olds\": [\"10-year-\", \"olds\"],\n",
    "        \"100-seat\": [\"100-\", \"seat\"],\n",
    "        \"51-year-old\": [\"51-year-\", \"old\"],\n",
    "        \"el-Haite\": [\"el-\", \"Haite\"],\n",
    "        \"6.30am;\": [\"6.30\", \"am;\"],\n",
    "        \"al-Mamun.\": [\"al-\", \"Mamun.\"],\n",
    "        \"Seven-year-old\": [\"Seven-year-\", \"old\"],\n",
    "    }\n",
    "    # For each duplicate in duplicates, updated the current word and the next word to the correct word\n",
    "    for idx, word in p_ias[duplicates].iterrows():\n",
    "        word_label = word[\"label\"]\n",
    "        assert isinstance(idx, int)\n",
    "        if word_label in word_parts:\n",
    "            if word_label == \"three-square-meter\":\n",
    "                p_ias.at[idx, \"label\"] = word_parts[word_label][level][0]\n",
    "                p_ias.at[idx + 1, \"label\"] = word_parts[word_label][level][1]\n",
    "            else:\n",
    "                p_ias.at[idx, \"label\"] = word_parts[word_label][0]\n",
    "                p_ias.at[idx + 1, \"label\"] = word_parts[word_label][1]\n",
    "        else:\n",
    "            print(\n",
    "                f\"Error: {folder_name}, {subject_id},{ias_path.stem}, {ias_path.name} has duplicate words not in word_parts: {word_label}\"\n",
    "            )\n",
    "\n",
    "    return p_ias, found_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_report_n = pd.read_csv(\n",
    "    base_path / \"Reports\" / \"n_reports\" / \"Output\" / \"n_trial_report.tsv\", sep=\"\\t\"\n",
    ")\n",
    "trial_report_p = pd.read_csv(\n",
    "    base_path / \"Reports\" / \"p_reports\" / \"Output\" / \"p_trial_report.tsv\", sep=\"\\t\"\n",
    ")\n",
    "trial_report = pd.concat([trial_report_n, trial_report_p], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing l42_2070\n",
      "Processing l59_547\n",
      "Processing l39_542\n",
      "Processing l56_522\n",
      "Processing l49_529\n",
      "Processing l9_536\n",
      "ose_3p_l1_l60_tower_st_mit_l1_latest_widthfix; Fixed 26 trials.\n",
      "Processing l59_485\n",
      "ose_2n_l1_l60_tower_st_lacclab_corrected; Fixed 2 trials.\n",
      "Processing l57_439\n",
      "Processing l53_431\n",
      "Processing l45_452\n",
      "Processing l46_453\n",
      "Processing l34_447\n",
      "Processing l44_412\n",
      "Processing l40_405\n",
      "Processing l41_451\n",
      "Processing l58_441\n",
      "Processing l37_395\n",
      "Processing l36_394\n",
      "Processing l54_461\n",
      "Processing l55_435\n",
      "Processing l48_422\n",
      "Processing l47_457\n",
      "Processing l50_460\n",
      "Processing l60_446\n",
      "Processing l35_450\n",
      "Processing l51_427\n",
      "Processing l52_428\n",
      "Processing l38_396\n",
      "Processing l43_411\n",
      "ose_3p_l1_l60_tower_st_fixed_lacc; Fixed 87 trials.\n",
      "Processing l11_525\n",
      "Processing l36_524\n",
      "ose_2p_l1_l60_tower_st_mit_l1_latest_widthfix; Fixed 5 trials.\n",
      "Processing l21_102\n",
      "Processing l22_103\n",
      "Processing l39_125\n",
      "Processing l60_190\n",
      "Processing l50_145\n",
      "Processing l16_96\n",
      "Processing l40_127\n",
      "Processing l8_268\n",
      "Processing l4_80\n",
      "Processing l41_129\n",
      "Processing l17_98\n",
      "Processing l24_105\n",
      "Processing l23_97\n",
      "Processing l55_157\n",
      "Processing l30_111\n",
      "Processing l14_94\n",
      "Processing l54_151\n",
      "Processing l58_177\n",
      "Processing l18_99\n",
      "Processing l56_159\n",
      "Processing l37_120\n",
      "Processing l53_150\n",
      "Processing l3_265\n",
      "Processing l15_95\n",
      "Processing l34_116\n",
      "Processing l36_119\n",
      "Processing l48_140\n",
      "Processing l6_83\n",
      "Processing l35_118\n",
      "Processing l45_135\n",
      "Processing l20_101\n",
      "Processing l51_147\n",
      "Processing l12_92\n",
      "Processing l42_290\n",
      "Processing l5_82\n",
      "Processing l52_149\n",
      "Processing l38_121\n",
      "Processing l13_93\n",
      "Processing l29_110\n",
      "Processing l19_100\n",
      "Processing l1_168\n",
      "Processing l27_108\n",
      "Processing l26_107\n",
      "Processing l57_170\n",
      "Processing l25_106\n",
      "Processing l10_269\n",
      "Processing l44_291\n",
      "Processing l31_112\n",
      "Processing l33_115\n",
      "Processing l2_174\n",
      "Processing l7_267\n",
      "Processing l43_131\n",
      "Processing l59_293\n",
      "Processing l11_91\n",
      "Processing l47_138\n",
      "Processing l46_137\n",
      "Processing l49_143\n",
      "ose_1n_l1_l60_tower_st; Fixed 137 trials.\n",
      "Processing l1_321\n",
      "Processing l4_326\n",
      "Processing l5_328\n",
      "Processing l3_325\n",
      "Processing l2_323\n",
      "Processing l32_502\n",
      "Processing l9_491\n",
      "ose_1n_l1_l60_tower_st_mit_l1_latest; Fixed 3 trials.\n",
      "Processing l18_526\n",
      "Processing l44_540\n",
      "Processing l34_512\n",
      "ose_1p_l1_l60_tower_st_mit_l1_latest_widthfix; Fixed 8 trials.\n",
      "Processing l37_534\n",
      "Processing l45_543\n",
      "Processing l55_519\n",
      "Processing l48_535\n",
      "Processing l14_527\n",
      "ose_2n_l1_l60_tower_st_mit_l1_latest_widthfix; Fixed 11 trials.\n",
      "Processing l54_516\n",
      "Processing l59_546\n",
      "Processing l33_515\n",
      "Processing l60_518\n",
      "ose_3n_l1_l60_tower_st_mit_l1_latest_widthfix; Fixed 14 trials.\n",
      "Processing l12_41\n",
      "Processing l9_38\n",
      "Processing l11_40\n",
      "Processing l10_39\n",
      "Processing l20_362\n",
      "Processing l5_332\n",
      "Processing l12_345\n",
      "Processing l11_344\n",
      "Processing l9_340\n",
      "Processing l32_390\n",
      "Processing l19_365\n",
      "Processing l28_380\n",
      "Processing l27_378\n",
      "Processing l17_356\n",
      "Processing l15_350\n",
      "Processing l8_339\n",
      "Processing l7_336\n",
      "Processing l23_368\n",
      "Processing l10_341\n",
      "Processing l4_331\n",
      "Processing l21_366\n",
      "Processing l22_367\n",
      "Processing l13_348\n",
      "Processing l16_354\n",
      "Processing l25_373\n",
      "Processing l26_375\n",
      "Processing l14_352\n",
      "Processing l31_388\n",
      "Processing l24_372\n",
      "Processing l30_386\n",
      "ose_3n_l1_l60_tower_st_fixed; Fixed 91 trials.\n",
      "Processing l41_295\n",
      "Processing l47_313\n",
      "Processing l40_294\n",
      "Processing l22_229\n",
      "Processing l37_273\n",
      "Processing l3_163\n",
      "Processing l20_226\n",
      "Processing l13_206\n",
      "Processing l1_155\n",
      "Processing l21_227\n",
      "Processing l23_235\n",
      "Processing l38_285\n",
      "Processing l15_214\n",
      "Processing l33_258\n",
      "Processing l9_195\n",
      "Processing l16_215\n",
      "Processing l45_306\n",
      "Processing l29_247\n",
      "Processing l7_183\n",
      "Processing l6_176\n",
      "Processing l14_212\n",
      "Processing l31_251\n",
      "Processing l42_297\n",
      "Processing l46_311\n",
      "Processing l4_169\n",
      "Processing l27_270\n",
      "Processing l30_249\n",
      "Processing l19_225\n",
      "Processing l18_222\n",
      "Processing l8_185\n",
      "Processing l2_160\n",
      "Processing l12_204\n",
      "Processing l43_302\n",
      "Processing l35_264\n",
      "Processing l39_289\n",
      "Processing l24_236\n",
      "Processing l32_255\n",
      "Processing l25_238\n",
      "Processing l44_305\n",
      "Processing l5_175\n",
      "Processing l48_315\n",
      "Processing l26_239\n",
      "Processing l28_242\n",
      "ose_2p_l1_l60_tower_st; Fixed 103 trials.\n",
      "Processing l17_493\n",
      "Processing l54_499\n",
      "Processing l10_494\n",
      "Processing l51_497\n",
      "Processing l53_498\n",
      "Processing l60_505\n",
      "Processing l58_503\n",
      "Processing l59_504\n",
      "Processing l55_500\n",
      "Processing l57_501\n",
      "Processing l56_487\n",
      "Processing l49_510\n",
      "ose_2p_l1_l60_tower_st_mit_l1_latest; Fixed 18 trials.\n",
      "Processing l11_342\n",
      "Processing l21_363\n",
      "Processing l12_343\n",
      "Processing l7_333\n",
      "Processing l22_364\n",
      "Processing l33_391\n",
      "Processing l24_370\n",
      "Processing l15_351\n",
      "Processing l26_376\n",
      "Processing l30_385\n",
      "Processing l27_377\n",
      "Processing l23_371\n",
      "Processing l13_347\n",
      "Processing l6_330\n",
      "Processing l20_361\n",
      "Processing l14_349\n",
      "Processing l19_359\n",
      "Processing l28_379\n",
      "Processing l18_358\n",
      "Processing l29_383\n",
      "Processing l16_353\n",
      "Processing l25_374\n",
      "Processing l31_387\n",
      "Processing l32_389\n",
      "Processing l17_355\n",
      "Processing l8_335\n",
      "Processing l10_338\n",
      "ose_3p_l1_l60_tower_st_fixed; Fixed 93 trials.\n",
      "Processing l28_495\n",
      "ose_1n_l1_l60_tower_st_mit_l2_latest; Fixed 2 trials.\n",
      "Processing l36_486\n",
      "Processing l56_489\n",
      "Processing l42_488\n",
      "ose_1p_l1_l60_tower_st_mit_l1_latest; Fixed 9 trials.\n",
      "Processing l46_2096\n",
      "ose_3n_l1_l60_tower_st_mit_l1_latest; Fixed 2 trials.\n",
      "Processing l60_198\n",
      "Processing l48_153\n",
      "Processing l2_65\n",
      "Processing l32_63\n",
      "Processing l49_154\n",
      "Processing l47_144\n",
      "Processing l39_122\n",
      "Processing l22_53\n",
      "Processing l15_45\n",
      "Processing l55_180\n",
      "Processing l23_54\n",
      "Processing l59_191\n",
      "Processing l14_43\n",
      "Processing l8_76\n",
      "Processing l17_48\n",
      "Processing l38_97\n",
      "Processing l30_61\n",
      "Processing l4_68\n",
      "Processing l20_51\n",
      "Processing l7_71\n",
      "Processing l51_158\n",
      "Processing l57_288\n",
      "Processing l5_69\n",
      "Processing l53_166\n",
      "Processing l50_156\n",
      "Processing l16_46\n",
      "Processing l25_56\n",
      "Processing l41_286\n",
      "Processing l6_70\n",
      "Processing l26_57\n",
      "Processing l46_141\n",
      "Processing l33_284\n",
      "Processing l35_85\n",
      "Processing l27_58\n",
      "Processing l40_123\n",
      "Processing l54_171\n",
      "Processing l3_66\n",
      "Processing l43_152\n",
      "Processing l24_55\n",
      "Processing l28_59\n",
      "Processing l37_87\n",
      "Processing l58_189\n",
      "Processing l13_42\n",
      "Processing l45_139\n",
      "Processing l21_52\n",
      "Processing l31_62\n",
      "Processing l19_50\n",
      "Processing l1_64\n",
      "Processing l29_60\n",
      "Processing l52_164\n",
      "ose_1p_l1_l60_tower_st; Fixed 119 trials.\n",
      "Processing l3_327\n",
      "Processing l6_329\n",
      "Processing l1_322\n",
      "Processing l2_324\n",
      "Processing l53_478\n",
      "Processing l52_477\n",
      "Processing l54_479\n",
      "Processing l58_483\n",
      "Processing l49_471\n",
      "Processing l51_475\n",
      "Processing l50_472\n",
      "Processing L56_481\n",
      "ose_2n_l1_l60_tower_st_lacc; Fixed 8 trials.\n",
      "Processing l52_2023\n",
      "Processing l50_474\n",
      "Processing l57_2095\n",
      "Processing l60_492\n",
      "ose_2n_l1_l60_tower_st_mit_l1_latest; Fixed 2 trials.\n",
      "Processing l56_438\n",
      "Processing l55_437\n",
      "Processing L41_408\n",
      "Processing l18_449\n",
      "Processing l35_399\n",
      "Processing l34_398\n",
      "Processing l57_463\n",
      "Processing l48_421\n",
      "Processing l38_402\n",
      "Processing l52_430\n",
      "Processing l39_404\n",
      "Processing l45_415\n",
      "Processing l37_401\n",
      "Processing l36_400\n",
      "Processing l50_425\n",
      "Processing l44_414\n",
      "Processing l58_466\n",
      "Processing l43_413\n",
      "Processing l42_459\n",
      "Processing l29_454\n",
      "Processing l40_458\n",
      "Processing l49_424\n",
      "Processing l53_432\n",
      "Processing l51_429\n",
      "Processing l47_420\n",
      "ose_3n_l1_l60_tower_st_fixed_lacc; Fixed 108 trials.\n",
      "Processing l34_473\n",
      "Processing l26_244\n",
      "Processing l18_220\n",
      "Processing l7_199\n",
      "Processing l25_243\n",
      "Processing l27_245\n",
      "Processing l44_312\n",
      "Processing l36_280\n",
      "Processing l2_165\n",
      "Processing l12_207\n",
      "Processing l39_299\n",
      "Processing l13_213\n",
      "Processing l42_308\n",
      "Processing l40_300\n",
      "Processing l23_234\n",
      "Processing l38_282\n",
      "Processing l21_232\n",
      "Processing l22_233\n",
      "Processing l31_257\n",
      "Processing l5_179\n",
      "Processing l28_246\n",
      "Processing l29_248\n",
      "Processing l24_241\n",
      "Processing l33_276\n",
      "Processing l47_319\n",
      "Processing l46_317\n",
      "Processing l3_167\n",
      "Processing l35_278\n",
      "Processing l8_200\n",
      "Processing l1_161\n",
      "Processing l43_310\n",
      "Processing l10_202\n",
      "Processing l9_201\n",
      "Processing l11_205\n",
      "Processing l34_277\n",
      "Processing l41_303\n",
      "Processing l20_253\n",
      "Processing l4_172\n",
      "Processing l32_263\n",
      "Processing l6_187\n",
      "Processing l16_218\n",
      "Processing l30_254\n",
      "Processing l15_210\n",
      "Processing l17_219\n",
      "Processing l19_230\n",
      "ose_2n_l1_l60_tower_st; Fixed 106 trials.\n",
      "Fixed trials saved to fixed_trials_20240319.csv\n"
     ]
    }
   ],
   "source": [
    "dry_run = False\n",
    "\n",
    "exp_folders_paths = exp_folders_path.glob(\"*\")\n",
    "fixed_trials = []\n",
    "all_trials_p_ias = []\n",
    "trial_report_subjects, non_trial_report_subjects = 0, 0\n",
    "mismatched_words = []\n",
    "for full_folder_name in exp_folders_paths:\n",
    "    folder_fixed_trials = []\n",
    "    folder_name = full_folder_name.name\n",
    "\n",
    "    for subject in (exp_folders_path / folder_name / \"results\").glob(\"[lL]*\"):\n",
    "        subject_id = subject.name\n",
    "\n",
    "        if subject_id not in trial_report[\"RECORDING_SESSION_LABEL\"].to_list():\n",
    "            print(f\"Skipping {subject_id} as it is not in the trial report\")\n",
    "            non_trial_report_subjects += 1\n",
    "            raise Exception\n",
    "        else:\n",
    "            print(f\"Processing {subject_id}\")\n",
    "            trial_report_subjects += 1\n",
    "\n",
    "        sub_trial_rep = trial_report[\n",
    "            trial_report[\"RECORDING_SESSION_LABEL\"] == subject_id\n",
    "        ][[\"batch\", \"article_id\", \"paragraph_id\", \"trial\", \"level\"]]\n",
    "\n",
    "        subject_dat = load_subject_dat(exp_folders_path, folder_name, subject_id)\n",
    "\n",
    "        ias_paths = get_sorted_ias_paths(\n",
    "            exp_folders_path, folder_name, subject_id, trial_report\n",
    "        )\n",
    "\n",
    "        skipped_ias = []\n",
    "        for ias_path in ias_paths:\n",
    "            # print(ias_path.name)\n",
    "            ias_data = load_ias(ias_path)\n",
    "            if len(ias_data) < 30:\n",
    "                skipped_ias.append(ias_path.stem)\n",
    "                continue\n",
    "\n",
    "            ias_data[\"ID\"] = (\n",
    "                ias_data.groupby(\"group\").cumcount() + 1\n",
    "            )  # for each group start the ID from 1\n",
    "\n",
    "            batch, article_id, paragraph_id, level = (\n",
    "                sub_trial_rep.loc[\n",
    "                    trial_report[\"trial\"]\n",
    "                    == str(\n",
    "                        get_trial_index(\n",
    "                            ias_path, trial_report=trial_report, subject_id=subject_id\n",
    "                        )\n",
    "                    )\n",
    "                ][[\"batch\", \"article_id\", \"paragraph_id\", \"level\"]]\n",
    "                .iloc[0]\n",
    "                .to_list()\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                p_ias = get_paragarph_ias(ias_data, subject_id=subject_id)\n",
    "            except Exception as e:\n",
    "                print(f\"Error: {e}\")\n",
    "                raise e\n",
    "\n",
    "            p_ias, found_duplicates = handle_long_words_going_down_line(\n",
    "                p_ias, folder_name, subject_id, ias_path, level=level\n",
    "            )\n",
    "            if found_duplicates:\n",
    "                folder_fixed_trials.append(\n",
    "                    (\n",
    "                        folder_name,\n",
    "                        subject_id,\n",
    "                        ias_path.stem,\n",
    "                        batch,\n",
    "                        article_id,\n",
    "                        paragraph_id,\n",
    "                        level,\n",
    "                        \"duplicates (hyphen at end of line) fixed\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            p_words = get_parag_words(\n",
    "                subject_dat=subject_dat,\n",
    "                ias_path=ias_path,\n",
    "                trial_report=trial_report,\n",
    "                subject_id=subject_id,\n",
    "            )\n",
    "\n",
    "            missing_words = get_missing_words(\n",
    "                p_words=p_words,\n",
    "                p_ias=p_ias,\n",
    "            )\n",
    "            if missing_words:\n",
    "                p_ias = add_missing_rows_to_ias(\n",
    "                    p_ias=p_ias, missing_words=missing_words\n",
    "                )\n",
    "\n",
    "                folder_fixed_trials.append(\n",
    "                    (\n",
    "                        folder_name,\n",
    "                        subject_id,\n",
    "                        ias_path.stem,\n",
    "                        batch,\n",
    "                        article_id,\n",
    "                        paragraph_id,\n",
    "                        level,\n",
    "                        \"11th row added\",\n",
    "                    )\n",
    "                )\n",
    "\n",
    "            ias_data = rebuild_ias_data(ias_data, p_ias)\n",
    "\n",
    "            if not dry_run:\n",
    "                ias_data.to_csv(ias_path, sep=\"\\t\", index=False, header=False)\n",
    "\n",
    "            # all_trials_p_ias.append(\n",
    "            #     (\n",
    "            #         subject_id,\n",
    "            #         batch,\n",
    "            #         article_id,\n",
    "            #         paragraph_id,\n",
    "            #         level,\n",
    "            #         # p_ias.drop(columns=[\"group\"]).reset_index(drop=True).to_string(),\n",
    "            #         p_ias.to_string(\n",
    "            #             index=False,\n",
    "            #             columns=[\"left\", \"right\", \"label\"],\n",
    "            #             col_space={\"left\": 10, \"right\": 10},\n",
    "            #         ),\n",
    "            #     )\n",
    "            # )\n",
    "\n",
    "    fixed_trials.extend(folder_fixed_trials)\n",
    "    if folder_fixed_trials:\n",
    "        print(f\"{folder_name}; Fixed {len(folder_fixed_trials)} trials.\")\n",
    "\n",
    "if fixed_trials:\n",
    "    date_str = datetime.now().strftime(\n",
    "        \"%Y%m%d\"\n",
    "    )  # get current date as a string in the format YYYYMMDD\n",
    "    pd.DataFrame(\n",
    "        fixed_trials,\n",
    "        columns=[\n",
    "            \"folder\",\n",
    "            \"subject\",\n",
    "            \"trial\",\n",
    "            \"batch\",\n",
    "            \"article_id\",\n",
    "            \"paragraph_id\",\n",
    "            \"level\",\n",
    "            \"reason\",\n",
    "        ],\n",
    "    ).to_csv(f\"fixed_trials_{date_str}.csv\", index=False)\n",
    "\n",
    "    print(f\"Fixed trials saved to fixed_trials_{date_str}.csv\")\n",
    "\n",
    "# all_trials_p_ias_df = pd.DataFrame(\n",
    "#     all_trials_p_ias,\n",
    "#     columns=[\"subject_id\", \"batch\", \"article_id\", \"paragraph_id\", \"level\", \"p_ias\"],\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert trial_report_subjects == 360\n",
    "assert non_trial_report_subjects == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>level</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>Adv</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>326</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>Adv</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>Adv</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>204</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Adv</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>202</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>Adv</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>Ele</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>6</td>\n",
       "      <td>Ele</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>Ele</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Adv</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>329</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>Ele</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>330 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch article_id paragraph_id level  count\n",
       "120     2          1            5   Adv      3\n",
       "326     3          9            3   Adv      3\n",
       "206     2          8            6   Adv      3\n",
       "204     2          8            5   Adv      3\n",
       "202     2          8            4   Adv      2\n",
       "..    ...        ...          ...   ...    ...\n",
       "87      1          7            4   Ele      1\n",
       "207     2          8            6   Ele      1\n",
       "91      1          8            2   Ele      1\n",
       "98      1          9            1   Adv      1\n",
       "329     3          9            4   Ele      1\n",
       "\n",
       "[330 rows x 5 columns]"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_trials_p_ias_df[\n",
    "    [\"p_ias\", \"batch\", \"article_id\", \"paragraph_id\", \"level\"]\n",
    "].drop_duplicates().groupby(\n",
    "    [\"batch\", \"article_id\", \"paragraph_id\", \"level\"]\n",
    ").size().reset_index(name=\"count\").sort_values(\n",
    "    by=\"count\", ascending=False\n",
    ")  # ['count'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (\n",
    "    all_trials_p_ias_df[[\"p_ias\", \"batch\", \"article_id\", \"paragraph_id\", \"level\"]]\n",
    "    .drop_duplicates()\n",
    "    .groupby([\"batch\", \"article_id\", \"paragraph_id\", \"level\"])\n",
    "    .agg(list)\n",
    ")\n",
    "df[[\"col1\", \"col2\", \"col3\"]] = df[\"p_ias\"].apply(pd.Series)\n",
    "df.drop(columns=[\"p_ias\"], inplace=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from Levenshtein import distance\n",
    "\n",
    "\n",
    "def compute_distance(x, ind1, ind2):\n",
    "    if isinstance(x[ind1], str) and isinstance(x[ind2], str):\n",
    "        return int(distance(x[ind1], x[ind2]))\n",
    "    else:\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batch</th>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>level</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Ele</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>Adv</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Ele</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>Ele</td>\n",
       "      <td>9.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>Ele</td>\n",
       "      <td>13.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>230</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>Adv</td>\n",
       "      <td>756.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>231</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>Adv</td>\n",
       "      <td>798.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>232</th>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>Adv</td>\n",
       "      <td>801.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>233</th>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>Adv</td>\n",
       "      <td>804.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>234</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>Adv</td>\n",
       "      <td>817.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>235 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    batch article_id paragraph_id level      0\n",
       "0       3          9            1   Ele    1.0\n",
       "1       3          9            1   Adv    2.0\n",
       "2       3          9            2   Ele    9.0\n",
       "3       1          3            5   Ele    9.0\n",
       "4       2          3            4   Ele   13.0\n",
       "..    ...        ...          ...   ...    ...\n",
       "230     2          7            2   Adv  756.0\n",
       "231     2          2            2   Adv  798.0\n",
       "232     3          6            6   Adv  801.0\n",
       "233     3          9            2   Adv  804.0\n",
       "234     3          3            3   Adv  817.0\n",
       "\n",
       "[235 rows x 5 columns]"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.apply(\n",
    "    lambda x: compute_distance(x, \"col1\", \"col2\"), axis=1\n",
    ").sort_values().reset_index().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = (\n",
    "    all_trials_p_ias_df[[\"p_ias\", \"batch\", \"article_id\", \"paragraph_id\", \"level\"]]\n",
    "    .query(\"level == 'Adv' and batch =='2' and article_id =='8' and paragraph_id=='5'\")\n",
    "    .drop_duplicates()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p_ias</th>\n",
       "      <th>batch</th>\n",
       "      <th>article_id</th>\n",
       "      <th>paragraph_id</th>\n",
       "      <th>level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>429</th>\n",
       "      <td>left      right       label\\n       358 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8586</th>\n",
       "      <td>left      right       label\\n       358 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Adv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17765</th>\n",
       "      <td>left      right       label\\n       358 ...</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>Adv</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   p_ias batch article_id  \\\n",
       "429          left      right       label\\n       358 ...     2          8   \n",
       "8586         left      right       label\\n       358 ...     2          8   \n",
       "17765        left      right       label\\n       358 ...     2          8   \n",
       "\n",
       "      paragraph_id level  \n",
       "429              5   Adv  \n",
       "8586             5   Adv  \n",
       "17765            5   Adv  "
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "429\n",
      "8586\n",
      "17765\n"
     ]
    }
   ],
   "source": [
    "for i in x.p_ias.items():\n",
    "    # print(i[0], i[1])\n",
    "    print(i[0])\n",
    "    # save to text file each i1\n",
    "    with open(f\"p_ias_{i[0]}.txt\", \"w\") as file:\n",
    "        file.write(i[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "folder\n",
       "ose_1n_l1_l60_tower_st_mit_l1_latest         1\n",
       "ose_2n_l1_l60_tower_st_lacc                  3\n",
       "ose_2n_l1_l60_tower_st_lacclab_corrected     1\n",
       "ose_2n_l1_l60_tower_st_mit_l1_latest         1\n",
       "ose_2p_l1_l60_tower_st_mit_l1_latest         7\n",
       "ose_3n_l1_l60_tower_st_fixed_lacc           24\n",
       "ose_3n_l1_l60_tower_st_mit_l1_latest         1\n",
       "ose_3p_l1_l60_tower_st_fixed_lacc           22\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_trials_df = pd.read_csv(\"fixed_trials.csv\")\n",
    "fixed_trials_df.drop_duplicates(subset=[\"subject\"]).reset_index(drop=True).groupby(\n",
    "    \"folder\"\n",
    ").size()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "osg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
